---
title: "RDM: Bias & Reproducibility"
pagetitle: "RDM: Bias & Reproducibility"
output:
  html_document:
    code_folding: show # allows toggling of showing and hiding code. Remove if not using code.
    code_download: true # allows the user to download the source .Rmd file. Remove if not using code.
    includes:
      after_body: footer.html # include a custom footer.
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: false
---

:::note
A key question in evaluating a given study is, "How do I establish trust in the validity of a finding?" Good RDM practices can help to address this question.
:::

## Measuring Bias

Good RDM pratices help address issues related to identifying and measuring bias in research. 

In an undergraduate context, trust is usually a by product of some proxy measure: peer review, credentials, journal, authority (validated by a course or instructor), etc. As one moves through their undergraduate degree, they may also begin conducting more in depth analyses of articles: are the reported methods appropriate; do the points in the discussion reflect back on the reported findings appropriately, etc. And in some cases, they may be asked to comment on analytical approaches used, level of reporting, if a data availability statment is present, etc. In general, these explorations don't venture beyond the record of publication, however.

Advancing through an academic career, trust turns to being able to ensure that stated findings can be confirmed from the data and analytical approach used in much greater detail. Two key questions need to be addressed in this context:

* How do I measure or evaluate for bias in the study?
* Can I reproduce the study's findings?

These, in turn, raise two more interconnected questions:

* What do I need to be able to answer these questions?
* What does it mean to reproduce the findings of a study?

## Types of Bias

Any given study is subject to bias. These can be broadly grouped into 3 categories, but these are strongly interconnected.

**Structural biases** impact who gets to do research in the first place; built into the systems in which we live and work, these manifest in things like: who gets published; who gets invited to share their research; and who gets promotion and tenure. This is a deep and complex source of bias.

**Cognitive biases**, knowingly and unknowingly, impact how a study is planned and implemented, contributing to a specific paradigm in which the study is run alongside structural biases. These may impact not just the kinds of questions asked, but how these questions are asked and the approaches used to attempt to answer them.

**Systemic bias**, or error, is a byproduct of studying complex systems and the reality that studies ask questions about only a subset of the components of these systems; we can't simultaneously measure everything, so we resolve to recognize that there are unmeasured forces impacting our object of study as well as things such as measurement error.

All of these sources of bias can impact the validity of a study's findings. One way of measuring this validity is to attempt to reproduce a given study. This replication is in part the backbone of the idea that science can be self correcting; as more data are collected and more studies are conducted, the error introduced by all this bias become less significant. This is perhaps most true of systemic bias; self correction on structural and cognitive biases is ostensibly much more challenging to address and / or correct for.

## Reproducibility and Replicability

Reproducibility and Replicability exist on a continuum from evaluating study internal validity to evaluating study generalizability.

Addressing study validity through replication allows us to hone in on specific aspects of the biases introduced, and namely where it's introduced. This also allows us to articulate what is required for successful replication and what a definition of successful replication might be.

It is common to have reproducibiliyt and replicability be articulated as these two categories -- the former referring to reproducing the results reported in a paper, the latter to re-running the same study. However, in the context of RDM, and the importance of how the record of research activities is maintained, it is valuable to unpack this a bit. Here, we will break replication efforts into four broad categories:

* Computational
* Study results
* Methods
* Generalization

For the purposes of this workshop, we will be focusing on aspects related to computational reproducibility, but we'll explore the others here.

### Computational Reproducibility

This is the most basic and rudimentary form of reproducibility and only validates procedural elements of a study's findings. And yet, even this is very challenging to achieve.

Computational reproducibility involves being able to take the same data, same analysis tool, and same analysis pipeline to derive the same results. Using open data types (such as csv) and scripted instruction sets (like R), can enhance computational reproducibility.

Validating the computational reproducibility of a study consequently requires access to a study's data (generally the cleaned data), the requisite documentation to understand this data (readmes, data dictionaries), and analysis protocols (software, software versions, tests run and selected parameters on those tests).

A lack of computational reproducibility may simply be frustrating, it may result in failure to publish, and it may result in retraction if found to sufficiently invalidate the study.

:::note
**Frustrations**

A good example of a frustrating experience when using a graphical user interface tool that generally limits reproducibility is the case of using Excel for genomics data where it was discovered that a significant number of published research articles storing their data in Excel contained errors. Reporting on this and access to the published studies can be found on [Retration Watch](https://retractionwatch.com/2023/09/20/guest-post-genomics-has-a-spreadsheet-problem/).

**Addressing the Issue**

It is becoming increasingly common for publishers to ask researchers to include a statement about code and data availability to help address these issues and to help ensure that computational reproducibility can be measured. For example, PLOS asks for a data availability statement, and their requirements [can be found here](https://journals.plos.org/plosone/s/data-availability). However, simply providing data and code, or a statement about them does not guarantee computational reproducibility in any meaningful way.

**Taking Things a Step Further**

Some disciplines and journals are addressing this issue by hiring data editors -- people responsible for ensuring the copmutational reproducibility of a study submitted for publication. The journals of the Econometrics Society is a good example, requiring a full reproducibility package to be submitted at the time that an article is submitted for initial consideration. The full requirements of the journal can be found on their [Data Editor Website](https://www.econometricsociety.org/publications/es-data-editor-website).
:::

### Study Results Reproducibility

Study results reproducibility helps to address analytical choices made in the study, it works with the same data as the original study, ideally the data as collected, but runs its own analytical pipelines in pursuit of the same research question. Border line p-values and small effect sizes are often contradicted or invalidated in these replications.

Validating study results requires access to their data (ideally as collected), the requisite documentation to understand this data, the hypothesis being tested, and possibly the analytical methods used.

A lack of results reproducibility can call a given study into question and raise concerns about researcher bias.

:::note
An early example to test analytical approaches with the same data set was published in 2018 -- *Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results*, available at [https://doi.org/10.1177/2515245917747646](https://doi.org/10.1177/2515245917747646).

This has since been followed by similar investigations in other disciplines.
:::

### Methods reproducibility

Methods reproducibility attempts to replicate a study as designed. It involves using the same methods, but with new data and a new sample from the same population. It helps to address choices related to study implementation including measurement error and sampling bias. This kind of reproducibility helps to address systemic bias, and is the first tool in the toolbox of science's ability to self correct. This is also what most closely aligns with 'replication'.

Validating a study's methods requires access to the hypothesis being tested alongside detailed methods protocols; the more robust the documentation, the more closely matched the study design, the greater the homogeneity between studies, and the more utility these studies provide to systematic reviews and meta analyses, as tools to evaluate the state of evidence on a particular research question. The further utility of methods reproducibility studies is contingent on access to their data and analytical pipelines.

The goal is to iteratively attempt to validate the significance testing and grow the body of evidence and available data addressing a specific question. An inability to conduct methods replication calls into question researcher bias (often suspected, but unable to pinpoint exactly where it is as a result of a lack of documentation). This may invalidate the original study, or worse, result in wasted research dollars and researcher time.

:::note
It was this kind of replication that first emerged in response to Ioannidis' 2005 article *Why Most Published Research Findings Are False*, available at [https://doi.org/10.1371/journal.pmed.1004085](https://doi.org/10.1371/journal.pmed.1004085). 

These kinds of replication studies were started in Pyschology (see *Estimating the Reproducibility of Psychological Science* available at [https://doi.org/10.1126/science.aac4716](https://doi.org/10.1126/science.aac4716), with full documentation at [https://osf.io/ezcuj/](https://osf.io/ezcuj/)), but have been explored in other areas as well -- for example, cancer research -- see Reproducibility Project: Cancer Biology at [https://www.cos.io/rpcb](https://www.cos.io/rpcb).
:::

### Study Generalizability

This is the most robust category of validation, and involves addressing the same research question, but from a novel perspective. It involves re-considering choices related to study design including how to measure and how to define the population. It may also involve re-evaluating choices related to how the question will be approached, impacting choice of over all study design.

Findings reproducibility attempts to extend the validity of a study, or address issues related to structural or cognitive bias in the research process.

## Reproducibility & RDM

Each type of reproducibility requires increasingly robust implementation of RDM best practices. Computational reproducibility for quantitative aspects of a research study should ostensibly be a baseline part of the peer review process, and requires cleaned data and scripts as well as a reproducble environment -- something we'll look at tomorrow.

Moving up the line to validate findings, we then need access to data as collected, to detailed protocols and methods, and version controlled records to understand when and how things changed. In addition, things like clearly articulated positionality statements help to demonstrate how much thought is put into the structural and cognitive biases that might influence their work.

### Evaluation to Practice

Just as we need this level of transparency to evaluate a given study, we should strive to provide this same level transparency for our peers and to enhance our research practices. This process begins with a protocol that details why you're doing what you're doing, and how you intend to do it - a data management plan is a key piece of this and it is where this workshop begins. It then involves updating this protocol and DMP to reflect how the process actually unfolded (we can never account for every eventuality), and once data are in hand, documenting your exploratory approaches - using R and RMarkdown for example - then scripting your analysis, and finally depositing these 'artefacts' of the final output somewhere that allows for validation.

## Challenges to Good RDM Practices

The challenges to good RDM practices are multifaceted, but deeply engrained in the reward systems of academia, the funding models of funding agencies, and the publishing model of journal publishers.

Begining at the undergraduate level students are often awarded on 'finding the right answer' over identifying error and bias. In an undergraduate lab where sample sizes are small and conditions are error prone, epmhasizing the 'right answer' sets the building blocks for a culture of practice that is not first and foremost geared to reducing questionable research practices, encouraging reproducible research, and championing good research data management practices.

:::note
Questionable research practices (QRPs) derive from researcher degrees of freedom and not clearly documenting the seperation between exploratory and confirmatory research.

An example of the former includes p-hacking, where data are worked until statistical significance is achieved. This may be through how the data are manipulated during cleaning, how thresholds are set, deciding to collect more data after the fact etc.

An example of the latter includes HARKing -- hypothesizing after the results are known. When the data collected do not support the hypothesis being tested, alternative hypotheses are tested on the data and potentially reported on as the original point of inquiry.
:::

:::note
Researcher degrees of freedom is a coined term referring to all the decisions a researcher is at liberty to make in the process of data collection and analysis, whether it be defining an outlier, rounding a variable, grouping variables (choosing a bin size for age ranges for example), deciding to collect more data after having looked at the data, etc. See *False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant* available at [https://doi.org/10.1177/0956797611417632](https://doi.org/10.1177/0956797611417632).
:::

At the graduate level and above in one's academic career, the reward system continues to emphasize outputs through publication counts, h-indexes, and impact factors -- that is, how much are you publishing, and how much is it being cited in highly cited journals. As evidenced by the few journals that employ data editors, the data -- and their veracity as demonstrated through good RDM practices -- themselves are not the primary deciding factor on publication, but rather the novelty and statistical significance of what is reported in the article. The focus on the latter is generally to the detriment of process making measuring bias and reproducibility extremely challenging. See for example *Do Pressures to Publish Increase Scientists' Bias? An Empirical Support from US States Data* available at [https://doi.org/10.1371/journal.pone.0010271](https://doi.org/10.1371/journal.pone.0010271).

This is coupled with novel and statistically significant findings being favoured by publishers in general. Reporting on replication studies and publishing non-significant findings is both challenging and not as highly rewarded either by publication or ensuing citation. But the value of these activities to scientific research is significant. This practice is not equal across all fields with some disciplines being more prone to the issues than others. See for example *"Positive" Results Increase Down the Hierarchy of the Sciences* available at [https://doi.org/10.1371/journal.pone.0010068](https://doi.org/10.1371/journal.pone.0010068).

At the funging stage, replication studies are generally not favoured when compared to novel pursuits with the potential of significant near term impact. This is perhaps most evident in medical fields, where many reseach questions are never addressed or significant replication never undertaken because research funds are targetted in the direction of novel pursuits with impact that is more easily reported on. See for example *How a now-retracted study got published in the first place, leading to a $3.8 million NIH grant* reported on by [Retraction Watch](https://retractionwatch.com/2023/06/09/how-a-now-retracted-study-got-published-in-the-first-place-leading-to-a-3-8-million-nih-grant/) or the recent issues highlighted in Alzheimer research, [as reported in Science](https://www.science.org/content/article/potential-fabrication-research-images-threatens-key-theory-alzheimers-disease), *Blots on a field*.

These challenges all manifest in poor documentation (documentation of decision making processes takes time, reducing number of scholarly outputs), and increased study bias (less time spent checking cognitive biases, increase in p-hacking and HARKing related practices to find significance, less attention to influence of systemic biases).

## When Things Go Awry

Jumping Spiders
