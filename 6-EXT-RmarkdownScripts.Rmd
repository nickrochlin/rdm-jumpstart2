---
title: "RMarkdown, Scripts & IDEs"
pagetitle: "RMarkdown, Scripts & IDEs"
output:
  html_document:
    code_folding: show # allows toggling of showing and hiding code. Remove if not using code.
    code_download: true # allows the user to download the source .Rmd file. Remove if not using code.
    includes:
      after_body: footer.html # include a custom footer.
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: false
---

We've been working through many, possibly new concepts, over the last several days. We're going to revsit a few of these here in the hopes of cementing a couple of key pieces that connect to both best pracitces in researh data management and reproducible data and analysis workflows. We'll also introduce two additional tools in R that can help with computational reproduciblity.

## Base R vs Tidyverse

You'll recall fron day 2 that base R is the 'stock' install of R. Base R is generally designed to be backwards compatible. That is, as new versions are released, they try not to deprecate old code, and as such, the hope is that code written under an earlier version of R will still run under the newer version of R -- if you only ever write in base R, you stand a better chance of your code not breaking in future or on a different machine.

However, base R is a hodge podge, so to speak, that has been developped over time. As a result, there is a serious lack of consistency across the implementation of its base functions. This can make it non-intuitive, difficult to understand, difficult to employ, and often violate the principles of literate programming.

On the other hand, Tidyverse is meant to be fast, intuitive, and standardized. Function names describe what they do, function arguments follow known patterns, etc. There is a very specific design philosophy to all the packages in the Tidyverse. This makes it great for interactive programming and iterative development. But, it ostensibly stands a higher chance than base R that future versions of the packages will not be compatible with older versions and that it will likely break at some point in the future with updates.

Given this, consider what you need. In a large, complex environment, one might start with a scripting language prone to deprecation with updates to ideate, but implement in a stable low level language, and then optimize specific parts in machine code. Some projects may only warrant computational reproducibility for the next five years, and so this level of refinement and optimization is not necessary.

This balancing act will be a recurring theme.

## Scripts, Notebooks & Abstraction

We've talked a bit about the differences between scripts and spreadsheet applications -- this can be abstracted to talking about programming instructions versus using graphical user interface (GUI) environments, where a GUI limits reproducibility while also complicating doing repretative tasks -- repetitive tasks take a lot of time and are error prone. Whenever possible, **we want to avoid using a GUI when reproducibility is important**.

While we've mentioned scripts, we've almost exclusively been working in RMarkdown documents, not R scripts. And this warrants breaking down a bit. Each has their place within a research context and within the context of research data management. **One of the most important pieces of RDM is documentation** and a key peice of documentation is the process of decision making in how one works through their data whether that be in the cleaning or analytical stage of the process. This has been referred to as 'researcher degrees of freedom'<sup>1</sup>. That is, all the decisions a researcher is at liberty to make in this process, whether it be defining an outlier, rounding a variable, grouping variables (choosing a bin size for age ranges for example), deciding you need to collect more data after having looked at the data, etc.

An RMarkdown document, sometimes referred to as an electronic notebook, is an excellent platform by which to maintain a record of these decisions; as you articulate your steps in markdown in plain language, this is directly in line with the code that processes your data. When we talked earlier about literate programming -- the idea that programming instructions should be both human and machine interprettable and tell a human what you want the computer to do -- an RMarkdown document enhances this concept with its rich context. However, at a certain stage, this rich context may be more appropriately decoupled from the scripting process -- especially as we move in the direction of confirmatory analyses, where we define in advance how we'll be handling the data. In these instances, the context is more appropriately contained within some form of study registration. In such a situation, we can use a script, or a document that contains only our instruction set (R code), perhaps with a few comments, but no markdown.

:::{.note}
**Exploratory research is hypothesis generating, while confirmatory research is hypothesis testing**. It is rare that confirmatory research is every conducted fully independent of exploratory research, as confirmatory research often suggests other paths of inquiry. However, it is critical to clearly differentiate between the two and to document the two processes appropriately, generally where exploratory research is documented while doing the exploration and confirmatory research is documented in advance.
:::

## Increased System Complexity

It's important to note that as we move from a script, to an RMarkdown document, to a GUI we increase the complexity of the system(s) that we're working in. Essentially, there are more moving parts. We'll talk about this a bit more when we talk about dependencies, but it's worth noting here the concept of abstraction as it relates to balancing ease of use of a system with replication -- we'll touch on this again when we look at building visualizations in R. The layers of abstraction in a programming language, like R is, in a simplified form:

0s and 1s -> machine code -> low level programming -> high level programming -> interactive programming (ie scientific computing, scripting) -> electronic notebooks -> GUI applications

This abstraction can make it difficult to understand exactly what's happening especially as a novice user, but an intuitive framework can help you move between systems. When we write code in R this goes through several stages of processing, often first just with a check and balance to make sure the data will work with the function it's being fed into, it will then actually run in a lower level language (like C or Fortran), which has been converted into machine code, an even more rudimentary instruction set, which is then stored on disk as binary values and processed by your CPU.

## IDEs

An IDE -- Integrated Development Environment -- is a piece of software intended to enhance the process of writing and testing code. In the context of reproducibility, one of the most valuable aspects of an IDE is it's project management capabilities which allow you to work with relative file paths instead of an absolute path or a path defined by `setwd()`, both of which were introduced in day 2. We've already introduce R projects with the .Rproj file. When you open your project file, your root directory is your working directory, allowing you to port your project to a different machine without directory structures wreaking havoc on running your code.

With an R project, all files are still referenced in relation to the script or RMarkdown file that you're writing in, however. So, if your project is organized like this:

|-- project-folder/
    |-- readme.md
    |-- data/
        |-- my-data.csv
    |-- scripts/
        |-- my-script.R

In order to read in data from your script you need to go up one level, into `data/` and then identify your data -- `read_csv("../data/my-data.csv")`

There is an additional package, `here` to help you improve this workflow, by making it such that you call all files from the root directory as the primary reference point. This, among other things, allows you to move a script without impacting it's ability to locate the data. Perhaps most importantly, it also allows you to open a script without first opening the .Rproj file and still have it run. That being said, `here` should be used within an R project; that is, you should have an .Rproj file in the root of your project.

To use `here`, first install it:

```{r, eval=FALSE}
install.packages("here")
```

Then, load it into each of your .R or .Rmd files

```{r}
library(here)
```

When you call here, it reports the root directory of your R project

```{r}
here()
```

To use `here` to load data, you call `here` from within data reading function, and feed it a character string representing the directory or directories to follow from your root all the way to your file:

```{r, eval=FALSE}
read_csv(here("data", "my-data.csv"))
```

You can read more about the here package at [https://here.r-lib.org/](https://here.r-lib.org/)
